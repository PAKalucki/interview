#!/usr/bin/env python3

import os
import boto3
import json
import logging

AWS_REGION = \"${region}\"
BUCKET_NAME = \"${bucket}\"

client = boto3.client(\"s3\", region_name=AWS_REGION)
logging.basicConfig(level = logging.INFO)

# Find all files in input folder
# this is limited to up to 1000 objects
list_objects_response = client.list_objects_v2(
    Bucket=BUCKET_NAME, Prefix='input/')

for object in list_objects_response['Contents']:
    object_key = object['Key']
    logging.info(f\"Getting file {object_key} from s3\")
    file_name = os.path.basename(object_key)
    client.download_file(
        BUCKET_NAME, f\"{object_key}\", f\"/downloads/{file_name}\")

    logging.info(\"Processing the file\")

    try:
        with open(f\"/downloads/{file_name}\", \"r\") as json_file:
            data = json.load(json_file)

            data[\"status\"] = \"done\"

            with open(f\"/downloads/{file_name}\", \"w\") as json_file:
                json.dump(data, json_file)

            logging.info(\"Uploading to output in s3\")
            client.upload_file(
                f\"/downloads/{file_name}\", BUCKET_NAME, f\"output/{file_name}\")

    except:
        logging.error(\"Processing failed, uploading to error in s3\")
        client.upload_file(
            f\"/downloads/{file_name}\", BUCKET_NAME, f\"error/{file_name}\")
